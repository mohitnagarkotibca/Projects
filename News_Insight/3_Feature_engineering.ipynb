{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',1000)\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "le= LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('data/cleaned_news_dataset.csv')\n",
    "data=data[['file_name','category','content3']].copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y= np.ravel(data.drop(['content3','file_name','category'],axis=1))\n",
    "Y= data.drop(['content3','file_name'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X= np.ravel(data['content3'])\n",
    "X= data['content3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y= train_test_split(X,Y,test_size=0.15,random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \n",
    "We have to define the different parameters:\n",
    "\n",
    "    ngram_range: We want to consider both unigrams and bigrams.\n",
    "    max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "    min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "    max_features: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.to_pickle('data/train_x_text.pickle')\n",
    "val_x.to_pickle('data/val_x_text.pickle')\n",
    "train_y.to_pickle('data/train_y_text.pickle')\n",
    "val_y.to_pickle('data/val_y_text.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_y))\n",
    "len(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most word frequencies according to category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news=data['content3'].values\n",
    "target=data['category'].values\n",
    "dic={}\n",
    "for i,text in enumerate(news):\n",
    "    val_count= pd.Series(text.split(' ')).value_counts()\n",
    "    for word in val_count.index:\n",
    "        if (word,target[i]) not in dic.keys():\n",
    "            dic[(word,target[i])]= val_count[word]\n",
    "        if (word,target[i]) in dic.keys():\n",
    "            dic[(word,target[i])]= dic[(word,target[i])] + val_count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dic.items(),key= lambda x: x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dic.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= [word[0][0] for word in list(dic.items())]\n",
    "category= [word[0][1] for word in list(dic.items())]\n",
    "counts= [word[1] for word in list(dic.items())]\n",
    "\n",
    "df= pd.DataFrame({'Words':words,'category':category,'counts':counts})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g= df.groupby('category')\n",
    "business= df_g.get_group('business')\n",
    "tech= df_g.get_group('tech')\n",
    "entertainment= df_g.get_group('entertainment')\n",
    "sport = df_g.get_group('sport')\n",
    "politics = df_g.get_group('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_words= business[business['counts'].values>100]\n",
    "best_words.sort_values(by='counts',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig= go.Figure()\n",
    "cats=['business','tech','entertainment','sport','politics']\n",
    "\n",
    "num= 40\n",
    "for i,category in enumerate([business,tech,entertainment,sport,politics]):\n",
    "    best_words= category[category['counts'].values>100].sort_values(by='counts',ascending=False)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "                    x= best_words.index,\n",
    "                    y= best_words['counts'].values,\n",
    "                    mode='markers',\n",
    "                    name=cats[i],\n",
    "                    hovertext=category['Words']\n",
    "        )\n",
    "    )\n",
    "fig.update_layout(\n",
    "                xaxis={\n",
    "                    'tickmode':'array',\n",
    "                    'tickvals':[10,9000,17000,25000,33000],\n",
    "                      'ticktext':['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "                },\n",
    "                title='most frequent words in all categories',\n",
    "                xaxis_title='Category',\n",
    "                yaxis_title='count'\n",
    "                    \n",
    ")\n",
    "    \n",
    "    \n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target']= le.fit_transform(data['category'])\n",
    "le.transform(['business', 'entertainment', 'politics', 'sport', 'tech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "we can make a vector using CountVectorizer also but Many Youtube Scientist says tfIDF vectorizer is better, let's find out :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_range=(1,2)\n",
    "max_df=1.\n",
    "min_df=10\n",
    "max_features=300\n",
    "tf= TfidfVectorizer(\n",
    "    ngram_range= n_range, \n",
    "    max_df= max_df, \n",
    "    min_df= min_df,\n",
    "    max_features= max_features\n",
    "    )\n",
    "train = tf.fit_transform(train_x).todense()\n",
    "validation= tf.transform(val_x).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(train,columns= tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train)\n",
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target_category']= train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats=['business', 'tech', 'entertainment', 'sport', 'politics']\n",
    "fig= go.Figure()\n",
    "for i in range(len(cats)):\n",
    "    df_g= df.groupby('target_category').get_group(cats[i])\n",
    "    cols=list(df_g.columns)[:-1]\n",
    "    counts= df_g.loc[0:,cols].sum(axis=0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "                    x= counts.index[0:],\n",
    "                    y= np.log(counts.values)[0:],\n",
    "                    mode='markers',\n",
    "                    name= cats[i]\n",
    "        )\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding relevant features\n",
    "le.classes_\n",
    "z= [(i,c) for i,c in enumerate(le.classes_)]\n",
    "category_codes= dict(z)\n",
    "category_codes.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "for code,cat in category_codes.items():\n",
    "    features_chi2= chi2(train,train_y==code)[0]\n",
    "    s_values= np.argsort(features_chi2)\n",
    "    \n",
    "    feature_names= np.array(tf.get_feature_names())[s_values]\n",
    "    \n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    \n",
    "    print(cat,':')\n",
    "    print(' , '.join(unigrams[-6:]))\n",
    "    print(' , '.join(bigrams[-5:]))\n",
    "    print('-'*60)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/news_dataset_v3_FE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train).to_pickle('data/train.pickle')\n",
    "pd.DataFrame(train_y).to_pickle('data/train_y.pickle')\n",
    "\n",
    "pd.DataFrame(validation).to_pickle('data/test.pickle')\n",
    "pd.DataFrame(val_y).to_pickle('data/test_y.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
